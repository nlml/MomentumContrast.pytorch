{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {}\n",
    "s[\"mnist/moco\"] = '''Test set: Average loss: -5.9028, Accuracy: 8303/10000 (83%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:36<00:00, 16.38it/s]Train Epoch: 2 | Loss: 0.007706 | Moco: 0.074024 | Sup 0.000304 | Entmin 0.007706 | Walk 0.007706\n",
    "\n",
    "\n",
    "Test set: Average loss: -6.7347, Accuracy: 8385/10000 (84%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.92it/s]Train Epoch: 3 | Loss: 0.007508 | Moco: 0.073401 | Sup 0.000168 | Entmin 0.007508 | Walk 0.007508\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.2476, Accuracy: 8463/10000 (85%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 21.91it/s]Train Epoch: 4 | Loss: 0.007414 | Moco: 0.073007 | Sup 0.000113 | Entmin 0.007414 | Walk 0.007414\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.4641, Accuracy: 8488/10000 (85%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 16.08it/s]Train Epoch: 5 | Loss: 0.007354 | Moco: 0.072728 | Sup 0.000081 | Entmin 0.007354 | Walk 0.007354\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.6058, Accuracy: 8507/10000 (85%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 16.08it/s]Train Epoch: 6 | Loss: 0.007324 | Moco: 0.072534 | Sup 0.000070 | Entmin 0.007324 | Walk 0.007324\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.7339, Accuracy: 8515/10000 (85%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 22.34it/s]Train Epoch: 7 | Loss: 0.007295 | Moco: 0.072344 | Sup 0.000060 | Entmin 0.007295 | Walk 0.007295\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8379, Accuracy: 8539/10000 (85%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 16.08it/s]Train Epoch: 8 | Loss: 0.007272 | Moco: 0.072221 | Sup 0.000050 | Entmin 0.007272 | Walk 0.007272\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9492, Accuracy: 8564/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:36<00:00, 21.28it/s]Train Epoch: 9 | Loss: 0.007254 | Moco: 0.072087 | Sup 0.000045 | Entmin 0.007254 | Walk 0.007254\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9866, Accuracy: 8574/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 22.21it/s]Train Epoch: 10 | Loss: 0.007242 | Moco: 0.071992 | Sup 0.000043 | Entmin 0.007242 | Walk 0.007242\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0184, Accuracy: 8595/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 21.50it/s]Train Epoch: 11 | Loss: 0.007229 | Moco: 0.071896 | Sup 0.000039 | Entmin 0.007229 | Walk 0.007229\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0633, Accuracy: 8595/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 16.07it/s]Train Epoch: 12 | Loss: 0.007216 | Moco: 0.071807 | Sup 0.000035 | Entmin 0.007216 | Walk 0.007216\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0652, Accuracy: 8594/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.74it/s]Train Epoch: 13 | Loss: 0.007207 | Moco: 0.071726 | Sup 0.000034 | Entmin 0.007207 | Walk 0.007207\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1225, Accuracy: 8627/10000 (86%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.93it/s]Train Epoch: 14 | Loss: 0.007200 | Moco: 0.071673 | Sup 0.000032 | Entmin 0.007200 | Walk 0.007200\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1791, Accuracy: 8691/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.79it/s]Train Epoch: 15 | Loss: 0.007192 | Moco: 0.071611 | Sup 0.000031 | Entmin 0.007192 | Walk 0.007192\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1660, Accuracy: 8679/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.76it/s]Train Epoch: 16 | Loss: 0.007185 | Moco: 0.071554 | Sup 0.000029 | Entmin 0.007185 | Walk 0.007185\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.2074, Accuracy: 8704/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.77it/s]Train Epoch: 17 | Loss: 0.007179 | Moco: 0.071498 | Sup 0.000029 | Entmin 0.007179 | Walk 0.007179\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1568, Accuracy: 8703/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.84it/s]Train Epoch: 18 | Loss: 0.007173 | Moco: 0.071462 | Sup 0.000027 | Entmin 0.007173 | Walk 0.007173\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.2066, Accuracy: 8726/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.62it/s]Train Epoch: 19 | Loss: 0.007169 | Moco: 0.071410 | Sup 0.000028 | Entmin 0.007169 | Walk 0.007169\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.2004, Accuracy: 8725/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 21.57it/s]Train Epoch: 20 | Loss: 0.007162 | Moco: 0.071378 | Sup 0.000024 | Entmin 0.007162 | Walk 0.007162\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1840, Accuracy: 8745/10000 (87%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.58it/s]Train Epoch: 21 | Loss: 0.007159 | Moco: 0.071336 | Sup 0.000025 | Entmin 0.007159 | Walk 0.007159\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1966, Accuracy: 8777/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.73it/s]Train Epoch: 22 | Loss: 0.007155 | Moco: 0.071294 | Sup 0.000026 | Entmin 0.007155 | Walk 0.007155\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.2143, Accuracy: 8774/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 20.95it/s]Train Epoch: 23 | Loss: 0.007149 | Moco: 0.071248 | Sup 0.000024 | Entmin 0.007149 | Walk 0.007149\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1539, Accuracy: 8792/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.88it/s]Train Epoch: 24 | Loss: 0.007147 | Moco: 0.071229 | Sup 0.000024 | Entmin 0.007147 | Walk 0.007147\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1663, Accuracy: 8773/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.79it/s]Train Epoch: 25 | Loss: 0.007144 | Moco: 0.071199 | Sup 0.000024 | Entmin 0.007144 | Walk 0.007144\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1917, Accuracy: 8780/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.80it/s]Train Epoch: 26 | Loss: 0.007141 | Moco: 0.071175 | Sup 0.000023 | Entmin 0.007141 | Walk 0.007141\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1078, Accuracy: 8789/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:37<00:00, 15.81it/s]Train Epoch: 27 | Loss: 0.007137 | Moco: 0.071148 | Sup 0.000023 | Entmin 0.007137 | Walk 0.007137\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1241, Accuracy: 8814/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.65it/s]Train Epoch: 28 | Loss: 0.007135 | Moco: 0.071126 | Sup 0.000023 | Entmin 0.007135 | Walk 0.007135\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1326, Accuracy: 8815/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 20.92it/s]Train Epoch: 29 | Loss: 0.007133 | Moco: 0.071103 | Sup 0.000023 | Entmin 0.007133 | Walk 0.007133\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.1032, Accuracy: 8827/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.51it/s]Train Epoch: 30 | Loss: 0.007130 | Moco: 0.071081 | Sup 0.000022 | Entmin 0.007130 | Walk 0.007130\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0951, Accuracy: 8828/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.52it/s]Train Epoch: 31 | Loss: 0.007127 | Moco: 0.071058 | Sup 0.000021 | Entmin 0.007127 | Walk 0.007127\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0432, Accuracy: 8827/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 21.37it/s]Train Epoch: 32 | Loss: 0.007125 | Moco: 0.071028 | Sup 0.000022 | Entmin 0.007125 | Walk 0.007125\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0724, Accuracy: 8837/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.58it/s]Train Epoch: 33 | Loss: 0.007122 | Moco: 0.071005 | Sup 0.000021 | Entmin 0.007122 | Walk 0.007122\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0863, Accuracy: 8851/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.59it/s]Train Epoch: 34 | Loss: 0.007121 | Moco: 0.070986 | Sup 0.000022 | Entmin 0.007121 | Walk 0.007121\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0280, Accuracy: 8851/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.55it/s]Train Epoch: 35 | Loss: 0.007118 | Moco: 0.070965 | Sup 0.000021 | Entmin 0.007118 | Walk 0.007118\n",
    "\n",
    "\n",
    "Test set: Average loss: -8.0730, Accuracy: 8860/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.49it/s]Train Epoch: 36 | Loss: 0.007116 | Moco: 0.070944 | Sup 0.000021 | Entmin 0.007116 | Walk 0.007116\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9820, Accuracy: 8849/10000 (88%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.50it/s]Train Epoch: 37 | Loss: 0.007114 | Moco: 0.070931 | Sup 0.000021 | Entmin 0.007114 | Walk 0.007114\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9692, Accuracy: 8891/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:39<00:00, 15.37it/s]Train Epoch: 38 | Loss: 0.007113 | Moco: 0.070914 | Sup 0.000021 | Entmin 0.007113 | Walk 0.007113\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9535, Accuracy: 8864/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.55it/s]\n",
    "Train Epoch: 39 | Loss: 0.007112 | Moco: 0.070900 | Sup 0.000022 | Entmin 0.007112 | Walk 0.007112\n",
    "\n",
    "Test set: Average loss: -7.9712, Accuracy: 8857/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 21.00it/s]Train Epoch: 40 | Loss: 0.007109 | Moco: 0.070879 | Sup 0.000021 | Entmin 0.007109 | Walk 0.007109\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9107, Accuracy: 8855/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:39<00:00, 15.38it/s]Train Epoch: 41 | Loss: 0.007110 | Moco: 0.070874 | Sup 0.000022 | Entmin 0.007110 | Walk 0.007110\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9510, Accuracy: 8892/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.47it/s]Train Epoch: 42 | Loss: 0.007107 | Moco: 0.070859 | Sup 0.000021 | Entmin 0.007107 | Walk 0.007107\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9074, Accuracy: 8902/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.41it/s]Train Epoch: 43 | Loss: 0.007105 | Moco: 0.070835 | Sup 0.000022 | Entmin 0.007105 | Walk 0.007105\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9062, Accuracy: 8922/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.46it/s]Train Epoch: 44 | Loss: 0.007105 | Moco: 0.070840 | Sup 0.000021 | Entmin 0.007105 | Walk 0.007105\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.9152, Accuracy: 8905/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.52it/s]Train Epoch: 45 | Loss: 0.007104 | Moco: 0.070827 | Sup 0.000021 | Entmin 0.007104 | Walk 0.007104\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8706, Accuracy: 8920/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.40it/s]Train Epoch: 46 | Loss: 0.007102 | Moco: 0.070807 | Sup 0.000021 | Entmin 0.007102 | Walk 0.007102\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8970, Accuracy: 8917/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.51it/s]Train Epoch: 47 | Loss: 0.007100 | Moco: 0.070791 | Sup 0.000021 | Entmin 0.007100 | Walk 0.007100\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8333, Accuracy: 8928/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.57it/s]Train Epoch: 48 | Loss: 0.007099 | Moco: 0.070782 | Sup 0.000021 | Entmin 0.007099 | Walk 0.007099\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8400, Accuracy: 8941/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.40it/s]Train Epoch: 49 | Loss: 0.007098 | Moco: 0.070768 | Sup 0.000021 | Entmin 0.007098 | Walk 0.007098\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8162, Accuracy: 8911/10000 (89%)\n",
    "\n",
    "1.0 0.0 False\n",
    "100%|██████████| 600/600.0 [00:38<00:00, 15.50it/s]Train Epoch: 50 | Loss: 0.007098 | Moco: 0.070764 | Sup 0.000022 | Entmin 0.007098 | Walk 0.007098\n",
    "\n",
    "\n",
    "Test set: Average loss: -7.8563, Accuracy: 8952/10000 (90%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-69c9cf20b8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set: Average loss: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "def do_s(s, run_name):\n",
    "    losses = [eval(i.split(\", A\")[0]) for i in s.split(\"Test set: Average loss: \")[1:] if len(i)]\n",
    "    accus = [eval(i.split(\" (\")[0]) for i in s.split(\", Accuracy: \")[1:] if len(i)]\n",
    "    df = data.copy()\n",
    "    df = pd.concat([df] * 10, 0).iloc[:len(losses)]\n",
    "    df[\"loss\"] = losses\n",
    "    df[\"accuracy\"] = accus\n",
    "    save_path = os.path.join(\"logs\", run_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    RESULTS_TRAIN.to_csv(os.path.join(save_path, \"valid.csv\"))\n",
    "    df[[\"loss\", \"accuracy\"]].to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1            2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2            3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3            4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4            5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "5            6  0.007324   0.072534  0.000070     0.007324     0.007324\n",
      "6            7  0.007295   0.072344  0.000060     0.007295     0.007295\n",
      "7            8  0.007272   0.072221  0.000050     0.007272     0.007272\n",
      "8            9  0.007254   0.072087  0.000045     0.007254     0.007254\n",
      "9           10  0.007242   0.071992  0.000043     0.007242     0.007242\n",
      "10          11  0.007229   0.071896  0.000039     0.007229     0.007229\n",
      "11          12  0.007216   0.071807  0.000035     0.007216     0.007216\n",
      "12          13  0.007207   0.071726  0.000034     0.007207     0.007207\n",
      "13          14  0.007200   0.071673  0.000032     0.007200     0.007200\n",
      "14          15  0.007192   0.071611  0.000031     0.007192     0.007192\n",
      "15          16  0.007185   0.071554  0.000029     0.007185     0.007185\n",
      "16          17  0.007179   0.071498  0.000029     0.007179     0.007179\n",
      "17          18  0.007173   0.071462  0.000027     0.007173     0.007173\n",
      "18          19  0.007169   0.071410  0.000028     0.007169     0.007169\n",
      "19          20  0.007162   0.071378  0.000024     0.007162     0.007162\n",
      "20          21  0.007159   0.071336  0.000025     0.007159     0.007159\n",
      "21          22  0.007155   0.071294  0.000026     0.007155     0.007155\n",
      "22          23  0.007149   0.071248  0.000024     0.007149     0.007149\n",
      "23          24  0.007147   0.071229  0.000024     0.007147     0.007147\n",
      "24          25  0.007144   0.071199  0.000024     0.007144     0.007144\n",
      "25          26  0.007141   0.071175  0.000023     0.007141     0.007141\n",
      "26          27  0.007137   0.071148  0.000023     0.007137     0.007137\n",
      "27          28  0.007135   0.071126  0.000023     0.007135     0.007135\n",
      "28          29  0.007133   0.071103  0.000023     0.007133     0.007133\n",
      "29          30  0.007130   0.071081  0.000022     0.007130     0.007130\n",
      "30          31  0.007127   0.071058  0.000021     0.007127     0.007127\n",
      "31          32  0.007125   0.071028  0.000022     0.007125     0.007125\n",
      "32          33  0.007122   0.071005  0.000021     0.007122     0.007122\n",
      "33          34  0.007121   0.070986  0.000022     0.007121     0.007121\n",
      "34          35  0.007118   0.070965  0.000021     0.007118     0.007118\n",
      "35          36  0.007116   0.070944  0.000021     0.007116     0.007116\n",
      "36          37  0.007114   0.070931  0.000021     0.007114     0.007114\n",
      "37          38  0.007113   0.070914  0.000021     0.007113     0.007113\n",
      "38          39  0.007112   0.070900  0.000022     0.007112     0.007112\n",
      "39          40  0.007109   0.070879  0.000021     0.007109     0.007109\n",
      "40          41  0.007110   0.070874  0.000022     0.007110     0.007110\n",
      "41          42  0.007107   0.070859  0.000021     0.007107     0.007107\n",
      "42          43  0.007105   0.070835  0.000022     0.007105     0.007105\n",
      "43          44  0.007105   0.070840  0.000021     0.007105     0.007105\n",
      "44          45  0.007104   0.070827  0.000021     0.007104     0.007104\n",
      "45          46  0.007102   0.070807  0.000021     0.007102     0.007102\n",
      "46          47  0.007100   0.070791  0.000021     0.007100     0.007100\n",
      "47          48  0.007099   0.070782  0.000021     0.007099     0.007099\n",
      "48          49  0.007098   0.070768  0.000021     0.007098     0.007098\n",
      "49          50  0.007098   0.070764  0.000022     0.007098     0.007098\n",
      "50           1  0.003140   0.003140  0.003140     0.003140     0.003140\n",
      "51           2  0.000313   0.000313  0.000313     0.000313     0.000313\n",
      "52           3  0.000165   0.000165  0.000165     0.000165     0.000165\n",
      "53           4  0.000111   0.000111  0.000111     0.000111     0.000111\n",
      "54           5  0.000087   0.000087  0.000087     0.000087     0.000087\n",
      "55           6  0.000070   0.000070  0.000070     0.000070     0.000070\n",
      "56           7  0.000058   0.000058  0.000058     0.000058     0.000058\n",
      "57           8  0.000052   0.000052  0.000052     0.000052     0.000052\n",
      "58           9  0.000051   0.000051  0.000051     0.000051     0.000051\n",
      "59          10  0.000043   0.000043  0.000043     0.000043     0.000043\n",
      "valid\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1            2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2            3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3            4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4            5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "5            6  0.007324   0.072534  0.000070     0.007324     0.007324\n",
      "6            7  0.007295   0.072344  0.000060     0.007295     0.007295\n",
      "7            8  0.007272   0.072221  0.000050     0.007272     0.007272\n",
      "8            9  0.007254   0.072087  0.000045     0.007254     0.007254\n",
      "9           10  0.007242   0.071992  0.000043     0.007242     0.007242\n",
      "10          11  0.007229   0.071896  0.000039     0.007229     0.007229\n",
      "11          12  0.007216   0.071807  0.000035     0.007216     0.007216\n",
      "12          13  0.007207   0.071726  0.000034     0.007207     0.007207\n",
      "13          14  0.007200   0.071673  0.000032     0.007200     0.007200\n",
      "14          15  0.007192   0.071611  0.000031     0.007192     0.007192\n",
      "15          16  0.007185   0.071554  0.000029     0.007185     0.007185\n",
      "16          17  0.007179   0.071498  0.000029     0.007179     0.007179\n",
      "17          18  0.007173   0.071462  0.000027     0.007173     0.007173\n",
      "18          19  0.007169   0.071410  0.000028     0.007169     0.007169\n",
      "19          20  0.007162   0.071378  0.000024     0.007162     0.007162\n",
      "20          21  0.007159   0.071336  0.000025     0.007159     0.007159\n",
      "21          22  0.007155   0.071294  0.000026     0.007155     0.007155\n",
      "22          23  0.007149   0.071248  0.000024     0.007149     0.007149\n",
      "23          24  0.007147   0.071229  0.000024     0.007147     0.007147\n",
      "24          25  0.007144   0.071199  0.000024     0.007144     0.007144\n",
      "25          26  0.007141   0.071175  0.000023     0.007141     0.007141\n",
      "26          27  0.007137   0.071148  0.000023     0.007137     0.007137\n",
      "27          28  0.007135   0.071126  0.000023     0.007135     0.007135\n",
      "28          29  0.007133   0.071103  0.000023     0.007133     0.007133\n",
      "29          30  0.007130   0.071081  0.000022     0.007130     0.007130\n",
      "30          31  0.007127   0.071058  0.000021     0.007127     0.007127\n",
      "31          32  0.007125   0.071028  0.000022     0.007125     0.007125\n",
      "32          33  0.007122   0.071005  0.000021     0.007122     0.007122\n",
      "33          34  0.007121   0.070986  0.000022     0.007121     0.007121\n",
      "34          35  0.007118   0.070965  0.000021     0.007118     0.007118\n",
      "35          36  0.007116   0.070944  0.000021     0.007116     0.007116\n",
      "36          37  0.007114   0.070931  0.000021     0.007114     0.007114\n",
      "37          38  0.007113   0.070914  0.000021     0.007113     0.007113\n",
      "38          39  0.007112   0.070900  0.000022     0.007112     0.007112\n",
      "39          40  0.007109   0.070879  0.000021     0.007109     0.007109\n",
      "40          41  0.007110   0.070874  0.000022     0.007110     0.007110\n",
      "41          42  0.007107   0.070859  0.000021     0.007107     0.007107\n",
      "42          43  0.007105   0.070835  0.000022     0.007105     0.007105\n",
      "43          44  0.007105   0.070840  0.000021     0.007105     0.007105\n",
      "44          45  0.007104   0.070827  0.000021     0.007104     0.007104\n",
      "45          46  0.007102   0.070807  0.000021     0.007102     0.007102\n",
      "46          47  0.007100   0.070791  0.000021     0.007100     0.007100\n",
      "47          48  0.007099   0.070782  0.000021     0.007099     0.007099\n",
      "48          49  0.007098   0.070768  0.000021     0.007098     0.007098\n",
      "49          50  0.007098   0.070764  0.000022     0.007098     0.007098\n",
      "50           1  0.003140   0.003140  0.003140     0.003140     0.003140\n",
      "51           2  0.000313   0.000313  0.000313     0.000313     0.000313\n",
      "52           3  0.000165   0.000165  0.000165     0.000165     0.000165\n",
      "53           4  0.000111   0.000111  0.000111     0.000111     0.000111\n",
      "54           5  0.000087   0.000087  0.000087     0.000087     0.000087\n",
      "55           6  0.000070   0.000070  0.000070     0.000070     0.000070\n",
      "56           7  0.000058   0.000058  0.000058     0.000058     0.000058\n",
      "57           8  0.000052   0.000052  0.000052     0.000052     0.000052\n",
      "58           9  0.000051   0.000051  0.000051     0.000051     0.000051\n",
      "59          10  0.000043   0.000043  0.000043     0.000043     0.000043\n",
      "train\n",
      "     Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0             1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1             2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2             3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3             4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4             5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "..          ...       ...        ...       ...          ...          ...\n",
      "102          43  0.506599   0.506599  0.000003     0.506599     0.506599\n",
      "103          44  0.506595   0.506595  0.000003     0.506595     0.506595\n",
      "104          45  0.506602   0.506602  0.000003     0.506602     0.506602\n",
      "105          46  0.506610   0.506610  0.000004     0.506610     0.506610\n",
      "106          47  0.506610   0.506610  0.000004     0.506610     0.506610\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "valid\n",
      "     Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0             1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1             2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2             3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3             4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4             5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "..          ...       ...        ...       ...          ...          ...\n",
      "102          43  0.506599   0.506599  0.000003     0.506599     0.506599\n",
      "103          44  0.506595   0.506595  0.000003     0.506595     0.506595\n",
      "104          45  0.506602   0.506602  0.000003     0.506602     0.506602\n",
      "105          46  0.506610   0.506610  0.000004     0.506610     0.506610\n",
      "106          47  0.506610   0.506610  0.000004     0.506610     0.506610\n",
      "\n",
      "[107 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.612793   0.073600  0.000798     0.001499     0.612793\n",
      "1            2  0.593442   0.071960  0.000178     0.000524     0.593442\n",
      "2            3  0.592123   0.071871  0.000179     0.000412     0.592123\n",
      "3            4  0.590836   0.071772  0.000130     0.000353     0.590836\n",
      "4            5  0.589896   0.071662  0.000099     0.000310     0.589896\n",
      "5            6  0.589697   0.071629  0.000103     0.000292     0.589697\n",
      "6            7  0.589432   0.071625  0.000091     0.000279     0.589432\n",
      "7            8  0.589399   0.071614  0.000103     0.000266     0.589399\n",
      "8            9  0.589588   0.071671  0.000105     0.000276     0.589588\n",
      "9           10  0.588883   0.071600  0.000084     0.000239     0.588883\n",
      "10          11  0.588984   0.071676  0.000081     0.000243     0.588984\n",
      "11          12  0.589090   0.071684  0.000100     0.000234     0.589090\n",
      "12          13  0.588614   0.071609  0.000081     0.000217     0.588614\n",
      "13          14  0.588598   0.071543  0.000093     0.000210     0.588598\n",
      "14          15  0.588319   0.071520  0.000068     0.000212     0.588319\n",
      "15          16  0.588524   0.071514  0.000084     0.000215     0.588524\n",
      "16          17  0.587939   0.071564  0.000039     0.000202     0.587939\n",
      "17          18  0.588200   0.071544  0.000072     0.000194     0.588200\n",
      "18          19  0.588216   0.071579  0.000078     0.000186     0.588216\n",
      "19          20  0.587932   0.071565  0.000055     0.000185     0.587932\n",
      "20          21  0.588172   0.071616  0.000069     0.000188     0.588172\n",
      "21          22  0.588050   0.071542  0.000078     0.000176     0.588050\n",
      "22          23  0.587770   0.071514  0.000057     0.000173     0.587770\n",
      "23          24  0.587601   0.071555  0.000044     0.000167     0.587601\n",
      "24          25  0.587743   0.071557  0.000059     0.000165     0.587743\n",
      "25          26  0.587755   0.071593  0.000055     0.000167     0.587755\n",
      "26          27  0.587777   0.071528  0.000062     0.000168     0.587777\n",
      "27          28  0.587743   0.071547  0.000065     0.000160     0.587743\n",
      "28          29  0.587679   0.071535  0.000064     0.000156     0.587679\n",
      "29          30  0.587803   0.071577  0.000061     0.000167     0.587803\n",
      "30          31  0.587628   0.071549  0.000056     0.000158     0.587628\n",
      "31          32  0.587802   0.071543  0.000076     0.000155     0.587802\n",
      "32          33  0.587508   0.071515  0.000056     0.000151     0.587508\n",
      "33          34  0.587644   0.071572  0.000058     0.000156     0.587644\n",
      "34          35  0.587781   0.071559  0.000069     0.000159     0.587781\n",
      "35          36  0.587507   0.071535  0.000050     0.000154     0.587507\n",
      "36          37  0.587432   0.071478  0.000058     0.000146     0.587432\n",
      "37          38  0.587318   0.071517  0.000043     0.000146     0.587318\n",
      "38          39  0.587356   0.071536  0.000045     0.000146     0.587356\n",
      "39          40  0.587363   0.071593  0.000042     0.000144     0.587363\n",
      "40          41  0.587638   0.071628  0.000060     0.000147     0.587638\n",
      "41          42  0.587631   0.071580  0.000066     0.000145     0.587631\n",
      "42          43  0.587232   0.071524  0.000046     0.000135     0.587232\n",
      "43          44  0.587282   0.071469  0.000052     0.000139     0.587282\n",
      "44          45  0.587223   0.071435  0.000053     0.000135     0.587223\n",
      "45          46  0.587281   0.071489  0.000048     0.000140     0.587281\n",
      "46          47  0.587298   0.071508  0.000048     0.000140     0.587298\n",
      "47          48  0.587391   0.071470  0.000065     0.000136     0.587391\n",
      "48          49  0.587154   0.071440  0.000050     0.000132     0.587154\n",
      "49          50  0.587240   0.071467  0.000052     0.000135     0.587240\n",
      "valid\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.612793   0.073600  0.000798     0.001499     0.612793\n",
      "1            2  0.593442   0.071960  0.000178     0.000524     0.593442\n",
      "2            3  0.592123   0.071871  0.000179     0.000412     0.592123\n",
      "3            4  0.590836   0.071772  0.000130     0.000353     0.590836\n",
      "4            5  0.589896   0.071662  0.000099     0.000310     0.589896\n",
      "5            6  0.589697   0.071629  0.000103     0.000292     0.589697\n",
      "6            7  0.589432   0.071625  0.000091     0.000279     0.589432\n",
      "7            8  0.589399   0.071614  0.000103     0.000266     0.589399\n",
      "8            9  0.589588   0.071671  0.000105     0.000276     0.589588\n",
      "9           10  0.588883   0.071600  0.000084     0.000239     0.588883\n",
      "10          11  0.588984   0.071676  0.000081     0.000243     0.588984\n",
      "11          12  0.589090   0.071684  0.000100     0.000234     0.589090\n",
      "12          13  0.588614   0.071609  0.000081     0.000217     0.588614\n",
      "13          14  0.588598   0.071543  0.000093     0.000210     0.588598\n",
      "14          15  0.588319   0.071520  0.000068     0.000212     0.588319\n",
      "15          16  0.588524   0.071514  0.000084     0.000215     0.588524\n",
      "16          17  0.587939   0.071564  0.000039     0.000202     0.587939\n",
      "17          18  0.588200   0.071544  0.000072     0.000194     0.588200\n",
      "18          19  0.588216   0.071579  0.000078     0.000186     0.588216\n",
      "19          20  0.587932   0.071565  0.000055     0.000185     0.587932\n",
      "20          21  0.588172   0.071616  0.000069     0.000188     0.588172\n",
      "21          22  0.588050   0.071542  0.000078     0.000176     0.588050\n",
      "22          23  0.587770   0.071514  0.000057     0.000173     0.587770\n",
      "23          24  0.587601   0.071555  0.000044     0.000167     0.587601\n",
      "24          25  0.587743   0.071557  0.000059     0.000165     0.587743\n",
      "25          26  0.587755   0.071593  0.000055     0.000167     0.587755\n",
      "26          27  0.587777   0.071528  0.000062     0.000168     0.587777\n",
      "27          28  0.587743   0.071547  0.000065     0.000160     0.587743\n",
      "28          29  0.587679   0.071535  0.000064     0.000156     0.587679\n",
      "29          30  0.587803   0.071577  0.000061     0.000167     0.587803\n",
      "30          31  0.587628   0.071549  0.000056     0.000158     0.587628\n",
      "31          32  0.587802   0.071543  0.000076     0.000155     0.587802\n",
      "32          33  0.587508   0.071515  0.000056     0.000151     0.587508\n",
      "33          34  0.587644   0.071572  0.000058     0.000156     0.587644\n",
      "34          35  0.587781   0.071559  0.000069     0.000159     0.587781\n",
      "35          36  0.587507   0.071535  0.000050     0.000154     0.587507\n",
      "36          37  0.587432   0.071478  0.000058     0.000146     0.587432\n",
      "37          38  0.587318   0.071517  0.000043     0.000146     0.587318\n",
      "38          39  0.587356   0.071536  0.000045     0.000146     0.587356\n",
      "39          40  0.587363   0.071593  0.000042     0.000144     0.587363\n",
      "40          41  0.587638   0.071628  0.000060     0.000147     0.587638\n",
      "41          42  0.587631   0.071580  0.000066     0.000145     0.587631\n",
      "42          43  0.587232   0.071524  0.000046     0.000135     0.587232\n",
      "43          44  0.587282   0.071469  0.000052     0.000139     0.587282\n",
      "44          45  0.587223   0.071435  0.000053     0.000135     0.587223\n",
      "45          46  0.587281   0.071489  0.000048     0.000140     0.587281\n",
      "46          47  0.587298   0.071508  0.000048     0.000140     0.587298\n",
      "47          48  0.587391   0.071470  0.000065     0.000136     0.587391\n",
      "48          49  0.587154   0.071440  0.000050     0.000132     0.587154\n",
      "49          50  0.587240   0.071467  0.000052     0.000135     0.587240\n",
      "train\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1            2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2            3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3            4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4            5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "5            6  0.007324   0.072534  0.000070     0.007324     0.007324\n",
      "6            7  0.007295   0.072344  0.000060     0.007295     0.007295\n",
      "7            8  0.007272   0.072221  0.000050     0.007272     0.007272\n",
      "8            9  0.007254   0.072087  0.000045     0.007254     0.007254\n",
      "9           10  0.007242   0.071992  0.000043     0.007242     0.007242\n",
      "10          11  0.007229   0.071896  0.000039     0.007229     0.007229\n",
      "11          12  0.007216   0.071807  0.000035     0.007216     0.007216\n",
      "12          13  0.007207   0.071726  0.000034     0.007207     0.007207\n",
      "13          14  0.007200   0.071673  0.000032     0.007200     0.007200\n",
      "14          15  0.007192   0.071611  0.000031     0.007192     0.007192\n",
      "15          16  0.007185   0.071554  0.000029     0.007185     0.007185\n",
      "16          17  0.007179   0.071498  0.000029     0.007179     0.007179\n",
      "17          18  0.007173   0.071462  0.000027     0.007173     0.007173\n",
      "18          19  0.007169   0.071410  0.000028     0.007169     0.007169\n",
      "19          20  0.007162   0.071378  0.000024     0.007162     0.007162\n",
      "20          21  0.007159   0.071336  0.000025     0.007159     0.007159\n",
      "21          22  0.007155   0.071294  0.000026     0.007155     0.007155\n",
      "22          23  0.007149   0.071248  0.000024     0.007149     0.007149\n",
      "23          24  0.007147   0.071229  0.000024     0.007147     0.007147\n",
      "24          25  0.007144   0.071199  0.000024     0.007144     0.007144\n",
      "25          26  0.007141   0.071175  0.000023     0.007141     0.007141\n",
      "26          27  0.007137   0.071148  0.000023     0.007137     0.007137\n",
      "27          28  0.007135   0.071126  0.000023     0.007135     0.007135\n",
      "28          29  0.007133   0.071103  0.000023     0.007133     0.007133\n",
      "29          30  0.007130   0.071081  0.000022     0.007130     0.007130\n",
      "30          31  0.007127   0.071058  0.000021     0.007127     0.007127\n",
      "31          32  0.007125   0.071028  0.000022     0.007125     0.007125\n",
      "32          33  0.007122   0.071005  0.000021     0.007122     0.007122\n",
      "33          34  0.007121   0.070986  0.000022     0.007121     0.007121\n",
      "34          35  0.007118   0.070965  0.000021     0.007118     0.007118\n",
      "35          36  0.007116   0.070944  0.000021     0.007116     0.007116\n",
      "36          37  0.007114   0.070931  0.000021     0.007114     0.007114\n",
      "37          38  0.007113   0.070914  0.000021     0.007113     0.007113\n",
      "38          39  0.007112   0.070900  0.000022     0.007112     0.007112\n",
      "39          40  0.007109   0.070879  0.000021     0.007109     0.007109\n",
      "40          41  0.007110   0.070874  0.000022     0.007110     0.007110\n",
      "41          42  0.007107   0.070859  0.000021     0.007107     0.007107\n",
      "42          43  0.007105   0.070835  0.000022     0.007105     0.007105\n",
      "43          44  0.007105   0.070840  0.000021     0.007105     0.007105\n",
      "44          45  0.007104   0.070827  0.000021     0.007104     0.007104\n",
      "45          46  0.007102   0.070807  0.000021     0.007102     0.007102\n",
      "46          47  0.007100   0.070791  0.000021     0.007100     0.007100\n",
      "47          48  0.007099   0.070782  0.000021     0.007099     0.007099\n",
      "48          49  0.007098   0.070768  0.000021     0.007098     0.007098\n",
      "49          50  0.007098   0.070764  0.000022     0.007098     0.007098\n",
      "valid\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.010867   0.076864  0.003180     0.010867     0.010867\n",
      "1            2  0.007706   0.074024  0.000304     0.007706     0.007706\n",
      "2            3  0.007508   0.073401  0.000168     0.007508     0.007508\n",
      "3            4  0.007414   0.073007  0.000113     0.007414     0.007414\n",
      "4            5  0.007354   0.072728  0.000081     0.007354     0.007354\n",
      "5            6  0.007324   0.072534  0.000070     0.007324     0.007324\n",
      "6            7  0.007295   0.072344  0.000060     0.007295     0.007295\n",
      "7            8  0.007272   0.072221  0.000050     0.007272     0.007272\n",
      "8            9  0.007254   0.072087  0.000045     0.007254     0.007254\n",
      "9           10  0.007242   0.071992  0.000043     0.007242     0.007242\n",
      "10          11  0.007229   0.071896  0.000039     0.007229     0.007229\n",
      "11          12  0.007216   0.071807  0.000035     0.007216     0.007216\n",
      "12          13  0.007207   0.071726  0.000034     0.007207     0.007207\n",
      "13          14  0.007200   0.071673  0.000032     0.007200     0.007200\n",
      "14          15  0.007192   0.071611  0.000031     0.007192     0.007192\n",
      "15          16  0.007185   0.071554  0.000029     0.007185     0.007185\n",
      "16          17  0.007179   0.071498  0.000029     0.007179     0.007179\n",
      "17          18  0.007173   0.071462  0.000027     0.007173     0.007173\n",
      "18          19  0.007169   0.071410  0.000028     0.007169     0.007169\n",
      "19          20  0.007162   0.071378  0.000024     0.007162     0.007162\n",
      "20          21  0.007159   0.071336  0.000025     0.007159     0.007159\n",
      "21          22  0.007155   0.071294  0.000026     0.007155     0.007155\n",
      "22          23  0.007149   0.071248  0.000024     0.007149     0.007149\n",
      "23          24  0.007147   0.071229  0.000024     0.007147     0.007147\n",
      "24          25  0.007144   0.071199  0.000024     0.007144     0.007144\n",
      "25          26  0.007141   0.071175  0.000023     0.007141     0.007141\n",
      "26          27  0.007137   0.071148  0.000023     0.007137     0.007137\n",
      "27          28  0.007135   0.071126  0.000023     0.007135     0.007135\n",
      "28          29  0.007133   0.071103  0.000023     0.007133     0.007133\n",
      "29          30  0.007130   0.071081  0.000022     0.007130     0.007130\n",
      "30          31  0.007127   0.071058  0.000021     0.007127     0.007127\n",
      "31          32  0.007125   0.071028  0.000022     0.007125     0.007125\n",
      "32          33  0.007122   0.071005  0.000021     0.007122     0.007122\n",
      "33          34  0.007121   0.070986  0.000022     0.007121     0.007121\n",
      "34          35  0.007118   0.070965  0.000021     0.007118     0.007118\n",
      "35          36  0.007116   0.070944  0.000021     0.007116     0.007116\n",
      "36          37  0.007114   0.070931  0.000021     0.007114     0.007114\n",
      "37          38  0.007113   0.070914  0.000021     0.007113     0.007113\n",
      "38          39  0.007112   0.070900  0.000022     0.007112     0.007112\n",
      "39          40  0.007109   0.070879  0.000021     0.007109     0.007109\n",
      "40          41  0.007110   0.070874  0.000022     0.007110     0.007110\n",
      "41          42  0.007107   0.070859  0.000021     0.007107     0.007107\n",
      "42          43  0.007105   0.070835  0.000022     0.007105     0.007105\n",
      "43          44  0.007105   0.070840  0.000021     0.007105     0.007105\n",
      "44          45  0.007104   0.070827  0.000021     0.007104     0.007104\n",
      "45          46  0.007102   0.070807  0.000021     0.007102     0.007102\n",
      "46          47  0.007100   0.070791  0.000021     0.007100     0.007100\n",
      "47          48  0.007099   0.070782  0.000021     0.007099     0.007099\n",
      "48          49  0.007098   0.070768  0.000021     0.007098     0.007098\n",
      "49          50  0.007098   0.070764  0.000022     0.007098     0.007098\n",
      "train\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.612793   0.073600  0.000798     0.001499     0.612793\n",
      "1            2  0.593442   0.071960  0.000178     0.000524     0.593442\n",
      "2            3  0.592123   0.071871  0.000179     0.000412     0.592123\n",
      "3            4  0.590836   0.071772  0.000130     0.000353     0.590836\n",
      "4            5  0.589896   0.071662  0.000099     0.000310     0.589896\n",
      "..         ...       ...        ...       ...          ...          ...\n",
      "93          44  0.278006   0.278006  0.000039     0.000160     0.278006\n",
      "94          45  0.278169   0.278169  0.000043     0.000160     0.278169\n",
      "95          46  0.277980   0.277980  0.000047     0.000153     0.277980\n",
      "96          47  0.277527   0.277527  0.000034     0.000138     0.277527\n",
      "97          48  0.277827   0.277827  0.000041     0.000146     0.277827\n",
      "\n",
      "[98 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "    Unnamed: 0      loss  loss_moco  loss_sup  loss_entmin  loss_walker\n",
      "0            1  0.612793   0.073600  0.000798     0.001499     0.612793\n",
      "1            2  0.593442   0.071960  0.000178     0.000524     0.593442\n",
      "2            3  0.592123   0.071871  0.000179     0.000412     0.592123\n",
      "3            4  0.590836   0.071772  0.000130     0.000353     0.590836\n",
      "4            5  0.589896   0.071662  0.000099     0.000310     0.589896\n",
      "..         ...       ...        ...       ...          ...          ...\n",
      "93          44  0.278006   0.278006  0.000039     0.000160     0.278006\n",
      "94          45  0.278169   0.278169  0.000043     0.000160     0.278169\n",
      "95          46  0.277980   0.277980  0.000047     0.000153     0.277980\n",
      "96          47  0.277527   0.277527  0.000034     0.000138     0.277527\n",
      "97          48  0.277827   0.277827  0.000041     0.000146     0.277827\n",
      "\n",
      "[98 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorboardX\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from shutil import rmtree\n",
    "os.makedirs('tboard', exist_ok=True)\n",
    "rmtree('tboard')\n",
    "\n",
    "def write_data(ds, run, data, group):\n",
    "    writer = SummaryWriter(f\"tboard/{ds}/{run}/{group}\")\n",
    "    d = data.to_dict()\n",
    "    del d['Unnamed: 0']\n",
    "    for idx in range(data.shape[0]):\n",
    "        epoch = idx + 1\n",
    "        for k, v in d.items():\n",
    "            writer.add_scalar(f'{k}', v[idx], epoch)\n",
    "\n",
    "for ds in os.listdir('logs'):\n",
    "    ds_dir = os.path.join('logs', ds)\n",
    "    for run in os.listdir(ds_dir):\n",
    "        for train_or_test in [\"train\", \"valid\"]:\n",
    "            data = pd.read_csv(os.path.join(ds_dir, run, f\"{train_or_test}.csv\"))\n",
    "            print(train_or_test)\n",
    "            print(data)\n",
    "            write_data(ds, run, data, train_or_test)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
